# Inference Server connection settings
INFER_SERVER_URL=http://127.0.0.1:5006/v1/completions
INFER_SERVER_TIMEOUT=120

USE_INTEGRATED_VLLM=true

# model name (filepath for gguf, hf repo name for others)
# MODEL_NAME=canopylabs/orpheus-3b-0.1-ft
# MODEL_NAME=models/Orpheus-3b-FT-Q8_0.gguf
MODEL_NAME=models/Orpheus-3b-FT-Q4_K_M.gguf

# model parameters
TOP_P=0.9
MAX_TOKENS=5000
TEMPERATURE=0.6
SAMPLE_RATE=24000
DTPYE="auto"
MAX_NUM_BATCHED_TOKENS=512
MAX_NUM_SEQS=4

# Fast API server settings
SERVER_PORT=5005
SERVER_HOST=0.0.0.0

HF_HOME="/workspace/hf"
